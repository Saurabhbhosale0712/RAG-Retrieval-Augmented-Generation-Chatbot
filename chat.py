# -*- coding: utf-8 -*-
"""Chat

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aTLG5MKD6geJi1VJs-j_eB2Q39r7A83d
"""





"""## A Lightweight Multimodal AI Chatbot using Local LLM and Hybrid Data Sources"""



"""### Step 1: Install Required Libraries
Since we‚Äôre using Google Colab, install the necessary libraries:

pypdf ‚Äì Extract text from PDFs

transformers ‚Äì Use pre-trained language models

sentence-transformers ‚Äì Convert text into embeddings

chromadb ‚Äì Store embeddings in a vector database

langchain ‚Äì Make retrieval and LLM integration easy
"""

!pip install pypdf transformers sentence-transformers chromadb langchain



"""### Step 2: Upload and Read a Book/PDF
Colab allows file uploads using the files module.
"""



!pip install PyPDF2



"""### Extract text from the uploaded PDF:"""



import requests
from bs4 import BeautifulSoup
from google.colab import files
from PyPDF2 import PdfReader

# Function to extract text from uploaded PDFs (starting from page 3)
def extract_text_from_uploaded_pdfs():
    uploaded = files.upload()
    all_text = ""
    for filename in uploaded:
        try:
            reader = PdfReader(filename)
            for page in reader.pages[2:]:  # Skip first two pages
                text = page.extract_text()
                if text:
                    all_text += text + "\n"
        except Exception as e:
            print(f"‚ùå Could not read {filename}: {e}")
    return all_text

# Function to extract text from multiple URLs
def extract_text_from_urls():
    urls = input("Paste one or more URLs (comma separated): ").split(",")
    all_text = ""
    for url in urls:
        url = url.strip()
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.content, "html.parser")
            paragraphs = soup.find_all("p")
            for para in paragraphs:
                text = para.get_text().strip()
                if text:
                    all_text += text + "\n"
        except Exception as e:
            print(f"‚ùå Failed to extract from {url}: {e}")
    return all_text

# PDF text extraction using PyPDF2 directly from a specified file
def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with open(pdf_path, "rb") as file:
            reader = PdfReader(file)
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"‚ùå Error reading PDF: {e}")
    return text

# Prompt user to choose input method
print("üì• Choose your input source:")
print("1. Upload PDF(s)")
print("2. Enter URLs")
print("3. Both PDF(s) and URLs")
choice = input("Enter your choice (1/2/3): ").strip()

extracted_text = ""

if choice == "1":
    print("\nüìÑ Please upload your PDF file(s):")
    extracted_text = extract_text_from_uploaded_pdfs()

elif choice == "2":
    print("\nüåê Please enter the URL(s):")
    extracted_text = extract_text_from_urls()

elif choice == "3":
    print("\nüìÑ Upload PDF file(s) first:")
    extracted_text += extract_text_from_uploaded_pdfs()
    print("\nüåê Now enter the URL(s):")
    extracted_text += extract_text_from_urls()

else:
    print("‚ùå Invalid choice. Please enter 1, 2, or 3.")

# Print preview of extracted text (optional)
print("\n‚úÖ Preview of extracted text:")
print(extracted_text[:1000])  # Show first 1000 characters



"""### Step 3: Preprocess Text (NLP Cleaning)
Before converting text into embeddings, clean it:

* Lowercasing

* Removing special characters

* Tokenization & Lemmatization


"""

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK resources (only once)
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")
nltk.download("omw-1.4")
nltk.download("averaged_perceptron_tagger")
# Download the missing 'punkt_tab' resource
nltk.download('punkt_tab')

# Function to clean and preprocess extracted text
def clean_text(text):
    # Lowercase and remove special characters
    text = text.lower()
    text = re.sub(r"\W+", " ", text)

    # Tokenization
    tokens = word_tokenize(text)

    # Stopword removal
    stop_words = set(stopwords.words("english"))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return " ".join(tokens)

# Clean the previously extracted_text (from PDFs, URLs, or both)
cleaned_text = clean_text(extracted_text)

# Preview the cleaned result
print("\nüßº Cleaned Text Preview:")
print(cleaned_text[:1000])  # Show first 1000 characters



"""### Step 4: Convert Text into Embeddings & Store in a Vector DB
Use sentence-transformers to generate embeddings and store them in ChromaDB.
"""

from sentence_transformers import SentenceTransformer
import chromadb

model = SentenceTransformer("all-MiniLM-L6-v2")  # Efficient embedding model

# Initialize ChromaDB
chroma_client = chromadb.PersistentClient(path="chroma_db")  # Stores embeddings persistently
collection = chroma_client.get_or_create_collection(name="documents")

# Split text into chunks
chunk_size = 500
chunks = [cleaned_text[i:i + chunk_size] for i in range(0, len(cleaned_text), chunk_size)]

# Store embeddings
for i, chunk in enumerate(chunks):
    embedding = model.encode(chunk).tolist()
    collection.add(ids=[str(i)], embeddings=[embedding], metadatas=[{"text": chunk}])

print("Text chunks stored in Vector DB!")



"""### Step 5: Retrieve Most Relevant Chunks
Now, when a user asks a question, we retrieve the most relevant chunks from ChromaDB.
"""

def retrieve_relevant_chunks(query):
    query_embedding = model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=3)  # Get top 3 matches
    return [res["text"] for res in results["metadatas"][0]]

query = "What is the main topic of the book?"
retrieved_chunks = retrieve_relevant_chunks(query)
print("\n\n".join(retrieved_chunks))



"""### Step 6: Pass Retrieved Chunks to LLM for Response

Model Name | 	Size	| Accuracy	| RAM Need	| GPU Needed

1. GPT4All-J	 | 3‚Äì4B | 	Medium	| 8GB  | 	‚ùå

2. TinyLLaMA 1.1B	| 1B	| Low-Mid | 	4‚Äì6GB	|  ‚ùå
3. Mistral (Quantized) | 	7B	| Good | 	8GB+	| ‚ùå (with quantization)
4. Phi-2 (Microsoft) | 	2.7B	 | Great at reasoning  | 	8GB	‚ùå

### Install Required Libraries for Phi-2
"""

!pip install transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load tokenizer and model (ensure you have a GPU runtime in Colab)
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype=torch.float16, device_map="auto")

# Combine retrieved chunks into a context
context = "\n".join(retrieved_chunks)

# Prompt for LLM (RAG-style prompt)
prompt = f"""Use the following context to answer the question below:
Context:
{context}

Question:
{query}

Answer:"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, temperature=0.7)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print only the answer part
print(response.split("Answer:")[-1].strip())



"""1.  max_new_tokens=150

üëâ This means the model can generate up to 150 new words or tokens as a response.

2. do_sample=True

üëâ This tells the model to add randomness while generating the answer.

3.  temperature=0.7

üëâ Controls how random or focused the output should be.

* 0.0 = very focused and deterministic (always gives same answer)

* 1.0 = very random (can be creative but sometimes silly)

* 0.7 = a good balance (slightly creative but still accurate)

So with temperature=0.7,
"""



from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ‚úÖ Load both models only once before the loop
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")
phi_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", torch_dtype=torch.float16, device_map="auto")

# ‚úÖ Feedback log list
feedback_log = []

# ‚úÖ Start the question-answer loop
while True:
    query = input("Ask your question (or type 'byy'/'stop' to exit): ").strip()

    if query.lower() in ["byy", "stop"]:
        print("Thanks for using the chatbot! üëã")
        break

    # üîç Use embedding_model for encoding
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=3)
    retrieved_chunks = [res["text"] for res in results["metadatas"][0]]
    context = "\n".join(retrieved_chunks)

    # üß† Prompt template to keep answer clean
    prompt = f"""You are a helpful assistant. Based only on the context below, provide a concise and accurate answer to the question. Do not include anything else.

Context:
{context}

Question:
{query}

Answer:"""

    # ‚ú® Generate response
    inputs = tokenizer(prompt, return_tensors="pt").to(phi_model.device)
    outputs = phi_model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=False,  # more deterministic
        temperature=0.7,
        eos_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # üó£Ô∏è Extract and print just the answer part
    if "Answer:" in response:
        answer = response.split("Answer:")[-1].strip()
    else:
        answer = response.strip()

    print("\nü§ñ Chatbot Answer:", answer)
    print("-" * 60)

    # üìù Ask for feedback and log it
    feedback = input("üí¨ Was this answer helpful? (yes/no): ").strip().lower()
    feedback_log.append({
        "question": query,
        "answer": answer,
        "feedback": feedback
    })





"""### Save Feedback Log
At the end of the session, export feedback to JSON or CSV:
"""

import json
with open("feedback_log.json", "w") as f:
    json.dump(feedback_log, f, indent=4)



# üîç Evaluate chatbot helpfulness based on feedback
def evaluate_chatbot(feedback_log):
    total = len(feedback_log)
    positive = sum(1 for entry in feedback_log if entry["feedback"] == "yes")
    negative = total - positive
    accuracy = (positive / total) * 100 if total > 0 else 0

    print("\nüìä Chatbot Evaluation Summary:")
    print(f"Total Questions Asked: {total}")
    print(f"Helpful Responses: {positive}")
    print(f"Unhelpful Responses: {negative}")
    print(f"‚úÖ Helpfulness Accuracy: {accuracy:.2f}%")

# Run this after the main chat loop
evaluate_chatbot(feedback_log)

